{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "import os\n",
    "import time\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "# https://nbviewer.jupyter.org/github/kaushaltrivedi/bert-toxic-comments-multilabel/blob/master/toxic-bert-multilabel-classification.ipynb?source=post_page-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WD = os.getcwd()\n",
    "DATA_DIR = os.path.join(WD, 'data','mpst-movie-plot-synopses-with-tags','mpst_full_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14828, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(DATA_DIR)\n",
    "data = data.drop(['synopsis_source'],axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>title</th>\n",
       "      <th>plot_synopsis</th>\n",
       "      <th>tags</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0057603</td>\n",
       "      <td>I tre volti della paura</td>\n",
       "      <td>Note: this synopsis is for the orginal Italian...</td>\n",
       "      <td>cult, horror, gothic, murder, atmospheric</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt1733125</td>\n",
       "      <td>Dungeons &amp; Dragons: The Book of Vile Darkness</td>\n",
       "      <td>Two thousand years ago, Nhagruul the Foul, a s...</td>\n",
       "      <td>violence</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0033045</td>\n",
       "      <td>The Shop Around the Corner</td>\n",
       "      <td>Matuschek's, a gift store in Budapest, is the ...</td>\n",
       "      <td>romantic</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0113862</td>\n",
       "      <td>Mr. Holland's Opus</td>\n",
       "      <td>Glenn Holland, not a morning person by anyone'...</td>\n",
       "      <td>inspiring, romantic, stupid, feel-good</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0086250</td>\n",
       "      <td>Scarface</td>\n",
       "      <td>In May 1980, a Cuban man named Tony Montana (A...</td>\n",
       "      <td>cruelty, murder, dramatic, cult, violence, atm...</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     imdb_id                                          title  \\\n",
       "0  tt0057603                        I tre volti della paura   \n",
       "1  tt1733125  Dungeons & Dragons: The Book of Vile Darkness   \n",
       "2  tt0033045                     The Shop Around the Corner   \n",
       "3  tt0113862                             Mr. Holland's Opus   \n",
       "4  tt0086250                                       Scarface   \n",
       "\n",
       "                                       plot_synopsis  \\\n",
       "0  Note: this synopsis is for the orginal Italian...   \n",
       "1  Two thousand years ago, Nhagruul the Foul, a s...   \n",
       "2  Matuschek's, a gift store in Budapest, is the ...   \n",
       "3  Glenn Holland, not a morning person by anyone'...   \n",
       "4  In May 1980, a Cuban man named Tony Montana (A...   \n",
       "\n",
       "                                                tags  split  \n",
       "0          cult, horror, gothic, murder, atmospheric  train  \n",
       "1                                           violence  train  \n",
       "2                                           romantic   test  \n",
       "3             inspiring, romantic, stupid, feel-good  train  \n",
       "4  cruelty, murder, dramatic, cult, violence, atm...    val  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = data['tags'].str.split(', ')\n",
    "lens = split.str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cult', 'horror', 'gothic', ..., 'anti war', 'murder',\n",
       "       'christian film'], dtype='<U18')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.concatenate(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cult' 'horror' 'gothic' 'murder' 'atmospheric' 'violence' 'romantic'\n",
      " 'inspiring' 'stupid' 'feel-good' 'cruelty' 'dramatic' 'action' 'revenge'\n",
      " 'sadist' 'queer' 'flashback' 'mystery' 'suspenseful' 'neo noir' 'prank'\n",
      " 'psychedelic' 'tragedy' 'autobiographical' 'home movie'\n",
      " 'good versus evil' 'depressing' 'realism' 'boring' 'haunting'\n",
      " 'sentimental' 'paranormal' 'historical' 'storytelling' 'comedy' 'fantasy'\n",
      " 'philosophical' 'adult comedy' 'cute' 'entertaining' 'bleak' 'humor'\n",
      " 'plot twist' 'christian film' 'pornographic' 'insanity' 'brainwashing'\n",
      " 'sci-fi' 'dark' 'claustrophobic' 'psychological' 'melodrama'\n",
      " 'historical fiction' 'absurd' 'satire' 'alternate reality'\n",
      " 'alternate history' 'comic' 'grindhouse film' 'thought-provoking'\n",
      " 'clever' 'western' 'blaxploitation' 'whimsical' 'intrigue' 'allegory'\n",
      " 'anti war' 'avant garde' 'suicidal' 'magical realism' 'non fiction']\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "temp_df = pd.DataFrame({'imdb_id': np.repeat(data['imdb_id'].values, lens), \n",
    "                        'category': np.concatenate(split),\n",
    "                       'values': 1})\n",
    "\n",
    "print(temp_df['category'].unique())\n",
    "print(len(temp_df['category'].unique()))\n",
    "\n",
    "temp_df = temp_df.pivot(index='imdb_id', columns='category', values='values').fillna(0).reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>title</th>\n",
       "      <th>plot_synopsis</th>\n",
       "      <th>tags</th>\n",
       "      <th>split</th>\n",
       "      <th>absurd</th>\n",
       "      <th>action</th>\n",
       "      <th>adult comedy</th>\n",
       "      <th>allegory</th>\n",
       "      <th>alternate history</th>\n",
       "      <th>...</th>\n",
       "      <th>sentimental</th>\n",
       "      <th>storytelling</th>\n",
       "      <th>stupid</th>\n",
       "      <th>suicidal</th>\n",
       "      <th>suspenseful</th>\n",
       "      <th>thought-provoking</th>\n",
       "      <th>tragedy</th>\n",
       "      <th>violence</th>\n",
       "      <th>western</th>\n",
       "      <th>whimsical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0057603</td>\n",
       "      <td>I tre volti della paura</td>\n",
       "      <td>Note: this synopsis is for the orginal Italian...</td>\n",
       "      <td>cult, horror, gothic, murder, atmospheric</td>\n",
       "      <td>train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt1733125</td>\n",
       "      <td>Dungeons &amp; Dragons: The Book of Vile Darkness</td>\n",
       "      <td>Two thousand years ago, Nhagruul the Foul, a s...</td>\n",
       "      <td>violence</td>\n",
       "      <td>train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0033045</td>\n",
       "      <td>The Shop Around the Corner</td>\n",
       "      <td>Matuschek's, a gift store in Budapest, is the ...</td>\n",
       "      <td>romantic</td>\n",
       "      <td>test</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0113862</td>\n",
       "      <td>Mr. Holland's Opus</td>\n",
       "      <td>Glenn Holland, not a morning person by anyone'...</td>\n",
       "      <td>inspiring, romantic, stupid, feel-good</td>\n",
       "      <td>train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0086250</td>\n",
       "      <td>Scarface</td>\n",
       "      <td>In May 1980, a Cuban man named Tony Montana (A...</td>\n",
       "      <td>cruelty, murder, dramatic, cult, violence, atm...</td>\n",
       "      <td>val</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     imdb_id                                          title  \\\n",
       "0  tt0057603                        I tre volti della paura   \n",
       "1  tt1733125  Dungeons & Dragons: The Book of Vile Darkness   \n",
       "2  tt0033045                     The Shop Around the Corner   \n",
       "3  tt0113862                             Mr. Holland's Opus   \n",
       "4  tt0086250                                       Scarface   \n",
       "\n",
       "                                       plot_synopsis  \\\n",
       "0  Note: this synopsis is for the orginal Italian...   \n",
       "1  Two thousand years ago, Nhagruul the Foul, a s...   \n",
       "2  Matuschek's, a gift store in Budapest, is the ...   \n",
       "3  Glenn Holland, not a morning person by anyone'...   \n",
       "4  In May 1980, a Cuban man named Tony Montana (A...   \n",
       "\n",
       "                                                tags  split  absurd  action  \\\n",
       "0          cult, horror, gothic, murder, atmospheric  train     0.0     0.0   \n",
       "1                                           violence  train     0.0     0.0   \n",
       "2                                           romantic   test     0.0     0.0   \n",
       "3             inspiring, romantic, stupid, feel-good  train     0.0     0.0   \n",
       "4  cruelty, murder, dramatic, cult, violence, atm...    val     0.0     1.0   \n",
       "\n",
       "   adult comedy  allegory  alternate history  ...  sentimental  storytelling  \\\n",
       "0           0.0       0.0                0.0  ...          0.0           0.0   \n",
       "1           0.0       0.0                0.0  ...          0.0           0.0   \n",
       "2           0.0       0.0                0.0  ...          0.0           0.0   \n",
       "3           0.0       0.0                0.0  ...          0.0           0.0   \n",
       "4           0.0       0.0                0.0  ...          0.0           0.0   \n",
       "\n",
       "   stupid  suicidal  suspenseful  thought-provoking  tragedy  violence  \\\n",
       "0     0.0       0.0          0.0                0.0      0.0       0.0   \n",
       "1     0.0       0.0          0.0                0.0      0.0       1.0   \n",
       "2     0.0       0.0          0.0                0.0      0.0       0.0   \n",
       "3     1.0       0.0          0.0                0.0      0.0       0.0   \n",
       "4     0.0       0.0          0.0                0.0      0.0       1.0   \n",
       "\n",
       "   western  whimsical  \n",
       "0      0.0        0.0  \n",
       "1      0.0        0.0  \n",
       "2      0.0        0.0  \n",
       "3      0.0        0.0  \n",
       "4      0.0        0.0  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_separate = data.merge(temp_df, how='left', on='imdb_id')\n",
    "data_separate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9580105396338104"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inbalance class ratio\n",
    "1-(data_separate.iloc[:,5:].sum(axis=1)/71).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9489, 76), (2373, 76), (2966, 76))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = data_separate[data_separate['split'] == 'train']\n",
    "val_df = data_separate[data_separate['split'] == 'val']\n",
    "test_df = data_separate[data_separate['split'] == 'test']\n",
    "\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_transformers import *\n",
    "from pytorch_transformers.modeling_bert import BertPreTrainedModel\n",
    "from pytorch_transformers.optimization import AdamW\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# bert_config = BertConfig.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPSTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_seq_length, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def get_sample_features(self, sample):\n",
    "        tokenized_sample = self.tokenizer.tokenize(sample)\n",
    "        \n",
    "        tokenized_sample = [\"[CLS]\"] + tokenized_sample[:self.max_seq_length-2] + [\"[SEP]\"]\n",
    "    \n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokenized_sample)\n",
    "        segment_ids = [0] * len(input_ids)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (self.max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "        \n",
    "        assert len(input_ids) == self.max_seq_length\n",
    "        assert len(input_mask) == self.max_seq_length\n",
    "        assert len(segment_ids) == self.max_seq_length\n",
    "        \n",
    "        return input_ids, input_mask, segment_ids\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataframe.iloc[idx]['plot_synopsis']\n",
    "        label = self.dataframe.iloc[idx][5:]\n",
    "        \n",
    "        input_ids, input_mask, segment_ids = self.get_sample_features(sample)\n",
    "        \n",
    "        return torch.tensor(input_ids), torch.tensor(input_mask), torch.tensor(segment_ids), torch.tensor(label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMultiLabelClassification(torch.nn.Module):\n",
    "    def __init__(self, num_labels=71):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.layer = torch.nn.Linear(768,300)\n",
    "        self.classifier = torch.nn.Linear(300, num_labels)\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(71)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n",
    "                position_ids=None, head_mask=None):\n",
    "        \n",
    "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask, head_mask=head_mask)\n",
    "\n",
    "        # pooled output\n",
    "        pooled_output = outputs[1]\n",
    "        \n",
    "        x = self.dropout(pooled_output)\n",
    "        x = torch.nn.functional.relu(self.layer(x))\n",
    "        logits = self.classifier(x)\n",
    "        logits = self.batchnorm(logits)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits\n",
    "        \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloaders, model, optimizer, criterion, scheduler, num_epochs=2):\n",
    "    since = time.time()\n",
    "    step_sizes = {'train': len(dataloaders['train']), \n",
    "                     'valid': len(dataloaders['valid'])}\n",
    "    \n",
    "    weight = torch.tensor([0.1, 0.9])\n",
    "\n",
    "    for epoch in tnrange(int(num_epochs), desc=\"Epoch\"):\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0\n",
    "            running_acc = 0\n",
    "        \n",
    "            for step, batch in enumerate(tqdm_notebook(dataloaders[phase], desc=phase)):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, input_mask, segment_ids, label_ids = batch\n",
    "                \n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "                sigmoid = F.sigmoid(logits)\n",
    "\n",
    "                loss = criterion(sigmoid, label_ids)\n",
    "                \n",
    "                label_weight = weight[label_ids.data.view(-1).long()].view_as(label_ids).to(device)\n",
    "                weighted_loss = loss * label_weight\n",
    "                weighted_loss_average = weighted_loss.mean()\n",
    "                \n",
    "                running_loss += weighted_loss_average.item()\n",
    "                \n",
    "                sigmoid_numpy = sigmoid.detach().cpu().numpy()\n",
    "                labels_numpy = label_ids.detach().cpu().numpy()\n",
    "    \n",
    "                acc = jaccard_score(labels_numpy, sigmoid_numpy.round(), average='samples')\n",
    "                running_acc += acc\n",
    "        \n",
    "                print(weighted_loss_average.item(), acc, sigmoid_numpy.round().sum(axis=1))\n",
    "    \n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    weighted_loss_average.backward()\n",
    "                    optimizer.step()\n",
    "#                     scheduler.step()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss / step_sizes[phase]\n",
    "                train_acc = running_acc / step_sizes[phase]\n",
    "            else:\n",
    "                valid_loss = running_loss / step_sizes[phase]\n",
    "                valid_acc = running_acc / step_sizes[phase]\n",
    "                \n",
    "                print('Epoch [{}/{}] train loss: {:.4f} acc: {:.4f} ' \n",
    "              'valid loss: {:.4f} acc: {:.4f}'.format(\n",
    "                epoch+1, num_epochs,\n",
    "                train_loss, train_acc, \n",
    "                valid_loss, valid_acc))\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MPSTDataset(train_df, bert_tokenizer, 128)\n",
    "train_dl = DataLoader(train_ds,batch_size=16, shuffle=True)\n",
    "\n",
    "val_ds = MPSTDataset(val_df, bert_tokenizer, 256)\n",
    "val_dl = DataLoader(val_ds,batch_size=16, shuffle=True)\n",
    "\n",
    "dloaders = {'train':train_dl, 'valid':val_dl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 10\n",
    "# LEARNING_RATE = 3e-4\n",
    "# ADAM_EPSILON = 1e-6\n",
    "# WARMUP_STEPS = 0\n",
    "\n",
    "# t_total= len(train_dl) * EPOCHS\n",
    "\n",
    "# model = BertForMultiLabelClassification.from_pretrained(\"bert-base-uncased\")\n",
    "# model.to(device)\n",
    "\n",
    "# criterion = BCEWithLogitsLoss()\n",
    "\n",
    "# param_optimizer = list(model.named_parameters())\n",
    "# no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "#     ]\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, eps=ADAM_EPSILON)\n",
    "# scheduler = WarmupLinearSchedule(optimizer, warmup_steps=WARMUP_STEPS, t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\AppData\\Local\\Continuum\\miniconda3\\envs\\main\\lib\\site-packages\\torch\\nn\\_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model = BertForMultiLabelClassification()\n",
    "model.freeze_bert_encoder()\n",
    "\n",
    "criterion = torch.nn.BCELoss(reduce=False)\n",
    "\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a97ebd27c024a398505db785f6261b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c6fd8e55ab4534966cab761d8f34c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=594, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\AppData\\Local\\Continuum\\miniconda3\\envs\\main\\lib\\site-packages\\torch\\nn\\functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.9000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        ...,\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.9000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000]])\n",
      "0.09277629107236862\n",
      "0.019466108831568907 [37. 37. 36. 37. 32. 37. 34. 33. 37. 31. 39. 35. 40. 34. 32. 42.]\n",
      "tensor([[0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.9000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        ...,\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000]])\n",
      "0.0955950915813446\n",
      "0.03800519068393488 [32. 34. 41. 37. 37. 45. 34. 38. 28. 29. 34. 35. 32. 36. 37. 35.]\n",
      "tensor([[0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.9000, 0.1000, 0.1000],\n",
      "        ...,\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000]])\n",
      "0.0979410856962204\n",
      "0.03154807546254915 [45. 33. 33. 32. 25. 35. 34. 35. 38. 34. 38. 38. 33. 35. 33. 39.]\n",
      "tensor([[0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        ...,\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000]])\n",
      "0.09435267001390457\n",
      "0.035674102978174 [32. 40. 33. 34. 41. 34. 40. 32. 38. 39. 29. 33. 42. 31. 37. 34.]\n",
      "tensor([[0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        ...,\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.9000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000]])\n",
      "0.10558176040649414\n",
      "0.06008113560008438 [30. 32. 33. 36. 32. 45. 35. 33. 38. 37. 37. 32. 40. 32. 39. 40.]\n",
      "tensor([[0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.9000, 0.1000, 0.1000],\n",
      "        ...,\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000]])\n",
      "0.09130950272083282\n",
      "0.039624125881414174 [28. 35. 33. 37. 34. 36. 29. 43. 37. 36. 34. 28. 40. 32. 35. 31.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-48454507bdb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training time: {:10f} minutes'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-117-5627f2fb0900>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(dataloaders, model, optimizer, criterion, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[0msigmoid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\main\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-82-3b16647229f6>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, attention_mask, labels, position_ids, head_mask)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n\u001b[1;32m---> 15\u001b[1;33m                             attention_mask=attention_mask, head_mask=head_mask)\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# pooled output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\main\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\main\\lib\\site-packages\\pytorch_transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, attention_mask, position_ids, head_mask)\u001b[0m\n\u001b[0;32m    707\u001b[0m         encoder_outputs = self.encoder(embedding_output,\n\u001b[0;32m    708\u001b[0m                                        \u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m                                        head_mask=head_mask)\n\u001b[0m\u001b[0;32m    710\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\main\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\main\\lib\\site-packages\\pytorch_transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    435\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[0mlayer_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\main\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\main\\lib\\site-packages\\pytorch_transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m         \u001b[0mattention_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\main\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\main\\lib\\site-packages\\pytorch_transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_tensor, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[0mself_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# add attentions if we output them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\main\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\main\\lib\\site-packages\\pytorch_transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\main\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\main\\lib\\site-packages\\torch\\nn\\modules\\dropout.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\main\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m    828\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[0;32m    829\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 830\u001b[1;33m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[0;32m    831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = train_model(dloaders, model, optimizer,criterion, scheduler=None, num_epochs=10)\n",
    "print('Training time: {:10f} minutes'.format((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_sample = train_ds[0]\n",
    "# single_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMultiLabelClassification()\n",
    "model.freeze_bert_encoder()\n",
    "\n",
    "criterion = BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0382, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0359, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0321, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0316, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0307, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0257, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0274, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0278, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0286, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0272, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0264, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0244, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0252, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0268, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0251, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0240, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0247, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0219, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0221, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0222, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0220, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0221, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0210, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0212, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0210, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0196, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0197, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0210, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0201, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0186, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0199, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0173, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0190, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0186, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0194, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0171, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0177, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0161, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0179, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0177, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0156, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0189, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0155, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0159, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0173, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0160, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0170, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0173, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0151, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0171, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0146, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0154, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0164, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0165, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0159, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0153, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0132, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0152, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0148, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0148, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0139, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0139, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0122, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0137, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0124, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0111, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0132, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0145, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0132, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0119, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0131, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0130, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0135, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0119, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0113, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0106, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0132, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0110, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0118, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0113, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0129, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0110, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0118, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0105, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0105, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0110, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0124, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0111, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0106, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0096, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0120, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0106, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0111, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0105, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0112, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0107, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0123, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0097, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0113, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0097, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    single_sample = tuple(t.to(device) for t in single_sample)\n",
    "    input_ids, input_mask, segment_ids, label_ids = single_sample\n",
    "    \n",
    "    logits = model(input_ids.unsqueeze(0), segment_ids.unsqueeze(0), input_mask.unsqueeze(0))\n",
    "    \n",
    "    loss = criterion(logits.view(-1, 71), label_ids.view(-1,71))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
